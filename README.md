# zhzAI - “小脑计划” (Project Cerebellum) 微模型铸造厂

<p align="center">
  <img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT">
</p>

<p align="center">
  <strong>告别大模型的“概率性”烦恼，为您的AI应用，亲手铸造一个100%可靠、极致高效的“微模型大脑”！</strong>
</p>

---

`model_foundry` 是一个独特的“宪法驱动”微模型铸造厂。它提供了一套完整的、从Python训练到任何语言（如Rust）部署的工业级全自动解决方案，并完美解决了`scikit-learn`复杂预处理器跨语言部署时最棘手的一致性难题。

## 为什么需要“小脑计划”？—— 在离线与低配的边缘，寻求智能的最优解

我们的终极目标是打造一个能**在任何设备上、完全离线运行**的个人AI助理。这个严苛的约束，意味着我们必须依赖像Qwen-0.6B这样极致小巧的本地模型。

但这带来了一个残酷的现实：**0.6B模型的能力，不足以独立、可靠地承担精准意图识别的重任。**

直接让它去判断用户的复杂指令，我们得到的是：

-   **灾难性的幻觉：** 在被要求“删除记忆”时，它可能会去执行“搜索”。
-   **高昂的推理成本：** 即便是小模型，每一次不必要的推理都在消耗宝贵的计算资源和用户的等待时间。
-   **不可预测的延迟：** 无法为用户的每一次交互提供稳定、瞬时的反馈。

> 一个真正可用的本地AI，绝不能是这样。

**“小脑计划”正是我们对这一困境给出的、最硬核的工程化回答。**

我们拒绝在“智能”和“性能”之间做妥协。我们选择**混合智能**，构建一个分工明确、协同作战的“大脑”：

1.  **让代码规则成为“反射弧”**  
    对于“删除”、“修改”等高风险指令，我们用100%确定的代码规则进行**零成本、零延迟**的瞬时捕获。这是系统不可动摇的“安全保险丝”。

2.  **让微模型成为“小脑”**  
    对于“这是个问题还是陈述？”这类海量的、模式化的判断，我们通过本仓库的流水线，**铸造**出专用的、仅有KB大小的微模型。它比0.6B的LLM**快数百倍、准数个数量级**，且行为高度可控。它以极低的资源开销，完美地弥补了小尺寸LLM在精准分类能力上的短板。

3.  **让LLM成为“大脑皮层”**  
    只有在真正需要深度理解、推理和生成时（例如，从长对话中提取核心事实），我们才调用宝贵的LLM资源，让它去完成自己最擅长、最高价值的工作。

**`model_foundry`就是您的“小脑”铸造工厂。** 它将赋予您为AI应用量产各种高精度、低成本“辅助处理器”的能力，让您的小尺寸本地模型，也能构建出一个真正稳定、高效、智能的混合AI系统。

---

## ✨ 核心特性

-   **蓝图驱动 (Blueprint-Driven):** 在`blueprints/`下定义新的模型“蓝图”（宪法+配置），即可轻松扩展，铸造全新架构的模型。
-   **一键式自动化引擎:** 由`main.py`统一调度，告别繁琐的手动分步执行。
-   **工业级数据提纯:** 包含精确去重与基于MinHash的近似去重，以及类别平衡策略。
-   **生产级模型导出:** 最终产出物是跨平台、高性能的`.onnx`格式模型，以及通过**Protocol Buffers (Protobuf)** 序列化的、类型安全且保证顺序的`.bin`预处理器数据，完美适配Rust、C++等高性能环境。
-   **闭环验证体验:** 提供交互式的“模型唤醒实验室”，让您在Python环境中就能立刻与自己铸造的模型进行对话，所见即所得。

---

## 🛠️ 架构与流水线 (Architecture & Pipeline)

本铸造厂采用“蓝图(Blueprints) + 引擎(Engine) + 主控(Controller)”的模块化架构。

| 阶段 (Stage)       | 核心引擎 (Engine Module)                | 职责 (Responsibility)                                                |
| ------------------ | --------------------------------------- | -------------------------------------------------------------------- |
| 1. 数据生成        | `foundry_engine/data_generator.py`      | 读取蓝图“宪法”，调用大模型API，高效、并发地挖掘海量原始训练数据。     |
| 2. 数据精炼        | `foundry_engine/data_refiner.py`        | 对原始数据进行深度清洗、去重、平衡和规范化，并产出详细的数据质量报告。 |
| 3. 模型训练        | `foundry_engine/trainers/*_trainer.py`  | 根据蓝图配置，调用相应的训练器，培育特定架构的模型。                 |
| 4. 预处理器导出    | `foundry_engine/exporter.py`            | 将Python预处理器中的核心数据，序列化为跨语言的`.bin` (Protobuf) 文件。 |

---

## 📦 最终产出物 (Artifacts)

流水线成功运行后，您将在`models/`目录中找到最终的、可部署到任何地方的产品。对于每个任务（如`is_question`），您会得到：

-   `is_question_classifier.onnx`: ONNX格式的模型文件。
-   `is_question_preprocessor.bin`: Protobuf格式的预处理器数据文件。

---

## ⚙️ 如何从零开始，铸造并唤醒你的第一个微模型

这是一个完整的、开箱即用的实践教程。

### 1. 环境设置

-   克隆本仓库。
-   **安装Python依赖**:
    ```bash
    # (推荐) 创建并激活一个新的虚拟环境
    # python -m venv .venv
    # source .venv/bin/activate
    
    # 安装所有必需的包 (我们使用uv，你也可以用pip)
    uv pip install -r requirements.txt
    ```
-   在项目根目录创建`.env`文件，并填入您的Gemini API密钥。您可以使用多个以逗号分隔的密钥来应对速率限制：
    ```
    GEMINI_API_KEYS=your_api_key_1,your_api_key_2
    ```

### 2. 执行一键式自动化生产线

告别繁琐的手动操作！现在，只需一条命令即可启动整个流水线。在`model_foundry/`根目录下执行：

```bash
# 这将为 "text_classification" 蓝图下的所有任务，完整地执行从数据生成到模型导出的所有步骤。
python main.py --blueprint text_classification
```

> **高级用法:** 您可以使用`--steps`参数来执行特定的步骤，例如，如果您只想重新训练和导出模型：
> `python main.py --blueprint text_classification --steps=train,export`

恭喜！您已经成功铸造出了两个高精度的微模型，它们的所有必需文件都已位于`models/`目录下。

### 3. 唤醒并与你的模型对话！

现在，是时候与您亲手创造的AI进行第一次对话了。我们为您准备了一个独立的“唤醒实验室”。

-   运行交互式测试脚本：
    
    ```bash
    python scripts/interactive_tester.py
    ```

-   **实践测试示例：**
    
    程序启动后，会首先进入“是否为问题”分类器的测试环节。
    
    ```
    --- 任务1: “是否为问题”分类器测试 ---
    请输入任意一句话，看看模型如何判断 (输入'q'退出):
    > 今天天气怎么样？
      模型预测 ->: **Question**

    > 帮我记一下明天的会议
      模型预测 ->: **Statement**

    > q
    ```
    
    输入`q`后，程序会自动进入“肯定/否定”分类器的测试环节。
    
    ```
    --- 任务2: “肯定/否定”分类器测试 ---
    现在，模拟AI向您确认，请输入您的回答 (输入'q'退出):
    > 没错，就这么办
      模型预测 ->: **Affirm**

    > 等等，先别
      模型预测 ->: **Deny**
    
    > q
    ```
